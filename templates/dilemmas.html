{% extends "base.html" %}

{% block body %}
<div id="startScreen" class="screen active">
    <h2>Critical Legal and Ethical Risks of Using AI in Recruitment</h2>
    <br>
    <p>When using any new technology, it's essential to consider its context and potential impact on regulatory compliance and ethical behavior. Here are some potential risks of using AI that talent acquisition teams should know:</p>
    <br>
    <h3>Potential for Bias</h3> 
    <br>
    <p>
        One of the most common criticisms of artificial intelligence is that it may contribute to bias in the hiring process. For example, when screening applicants, there is a concern that AI-powered filters could erroneously favor specific candidates based on their gender, race, age, or another characteristic. In 2018, a global tech company shut down its AI recruitment tool because it demonstrated a preference for male candidates over females.
    
        Though an estimated 59% percent of recruiters agree that introducing AI into the recruitment process will remove unconscious bias, AI tools must be appropriately trained for this to occur. Humans train AI to perform specific functions, so any unconscious biases held by people could be woven into tools for sourcing, screening, and assessing applicants.
    </p>
    <br>
    <img src="/static/bias.jpg" style="display: block; margin: 0 auto; height: 300px; width:500px;" alt="Bias">
    <br>
    <h3>Privacy Concerns</h3> 
    <br>
    <p>
        Using AI tools to screen resumes and store Personally Identifiable Information (PII), such as names, email addresses, and employment histories, can be risky with sufficient data protection protocols. Hackers and identity thieves could steal and misuse it, creating data privacy headaches for the organization and its candidates.
        
        Another potential issue is the improper collection and use of candidate data. For instance, AI tools that "scrape" or pull information from candidate social media profiles might infer personal attributes like gender identity, sexual orientation, or political views. If these details influence hiring decisions, the organization may risk lawsuits, reputational damage, and potentially overlooking qualified candidates.
    </p>
    <br>
    <h3>Federal and State Laws</h3> 
    <br>
    <img src="/static/privacy.jpg" style="float: left; width: 300px; height: 400px; margin-right: 20px;" alt="Privacy">
    <br>
    <p>
        As talent acquisition professionals are aware, several federal laws prevent discrimination in hiring practices, including the Americans with Disabilities Act (ADA) and the Equal Employment Opportunity (EEO) Act. Given the rise in AI recruitment tools, they are increasingly scrutinized for their potential to violate anti-discrimination laws. In 2022, the U.S. Justice Department and the Equal Employment Opportunity Commission issued a statement warning against using AI tools that could violate the ADA.
    
        State and municipal laws also regulate AI in recruiting. For example, the Illinois Artificial Intelligence Video Interview Act requires employers to take specific steps before conducting AI-assisted video interviews. In New York City, employers must now conduct periodic audits of their AI hiring tools to ensure they're free of bias and discrimination.
    </p>
    <br>
    <h3>Transparency with Candidates</h3> 
    <br>
    <p>
        More transparency in how organizations use AI for hiring may make candidates more skeptical about its value. Gallup research found that 85% of Americans are concerned about using AI for hiring decisions, so organizations can hurt their ability to hire by not being more forthcoming. Suppose candidates are misinformed about how AI is used or think it makes the hiring process unfair. In that case, they may be less likely to trust the organization and less interested in pursuing job opportunities there.
    </p>
    <br>
    <img src="/static/security.jpg" style="display: block; margin: 0 auto; height: 300px; width:500px;" alt="Security">
    <br>
    <h3>Candidate Consent and Data-Sharing Permission</h3> 
    <br>
    <p>
        Just as candidates may be more trusting of an organization that is transparent about its use of AI, they may also appreciate having more choice in how AI is used in the recruitment process. Without it, candidates may resent AI tools used to determine their candidacy and be concerned about how their personal data is used. Some municipalities share this concern and have passed laws requiring candidate consent. For example, in Maryland, employers must get permission from candidates before using AI-powered facial recognition technology in job interviews.
    </p>
    <br>
    <h3>Balancing Efficiency with Fairness</h3> 
    <br>
    <p>Though AI can automate several hiring activities, recruitment teams should ensure that the desire for efficiency doesn't overshadow the need for equity and fairness. For instance, over-reliance on AI for sourcing and screening could overlook diverse candidates and those with unique skills and experiences. It could also unfairly reject qualified candidates during the interview and assessment stage.
    Large language models (artificial intelligence designed to understand and generate human-like text by processing vast amounts of data) can "hallucinate," meaning they sometimes generate information that seems plausible but is incorrect or fabricated. In recruitment, this could lead to serious issues, such as inaccurately summarizing candidate qualifications, generating misleading job descriptions, or recommending unsuitable candidates. These hallucinations can also erode trust in AI-driven recruitment tools and result in poor hiring decisions.
    </p>
    <br>
    <p style="text-align: right;">(source: recruitics.com)</p>
</div>
{% endblock %}
