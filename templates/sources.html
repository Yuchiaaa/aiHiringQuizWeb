{% extends "base.html" %}

{% block body %}
<div id="startScreen" class="screen active">
    <h2>The content design of this website refers to the following sources(you may feel interested in):</h2>
    <br>
    <a href="https://heinonline.org/hol-cgi-bin/get_pdf.cgi?handle=hein.journals/cdozo41&section=47">
        Ajunwa, I. (2019). The paradox of automation as anti-bias intervention. Cardozo L. Rev., 41, 1671.
    </a>
    <p>
        Ajunwa critiques the assumption that AI hiring systems inherently reduce bias, arguing that automation can reinforce structural inequalities under the guise of efficiency. The paper links this dilemma to justice as fairness, warning that AI hiring often prioritises operational efficiency over fair opportunity and individual dignity, which conflicts with deontological ethics.
    </p>
    <br>
    <a href="https://www.ssoar.info/ssoar/handle/document/89808">
        Dinika, A. A., & Sloane, M. (2023). AI and Inequality in Hiring and Recruiting: A Field Scan. In Weizenbaum Conference" AI, Big Data, Social Media, and People on the Move" (pp. 1-13). DEU.
    </a>
    <p> 
        This field scan examines how AI recruitment tools can perpetuate existing inequalities by embedding ideological assumptions about productivity and meritocracy. The authors call for a critical examination of these tools to ensure they align with ethical standards that promote equity and justice.​
    </p>
    <br>
    <a href="https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2016.0360">
        Floridi, L., & Taddeo, M. (2016). What is data ethics?. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2083), 20160360.
    </a>
    <p>
        This paper presents the idea of data ethics and addresses how artificial intelligence systems, including recruiting algorithms, should treat personal data sensibly.  The writers go over the moral conundrums raised by digital era data collecting, permission, and privacy.
    </p>
    <br>
    <a href="https://link.springer.com/article/10.1007/s10676-021-09615-w">
        Fritts, M., & Cabrera, F. (2021). AI recruitment algorithms and the dehumanization problem. Ethics and Information Technology, 23, 791-801.
    </a>
    <p>
        This paper explores concerns that AI recruitment tools may dehumanize the hiring process by embedding values that prioritise efficiency over human judgement. The authors argue that such dehumanization reflects an underlying ideology that conflicts with ethical principles emphasising human dignity and fairness.
    </p>
    <br>
    <a href="https://journals.sagepub.com/doi/abs/10.1177/2053951718756684">
        Lee, M. K. (2018). Understanding perception of algorithmic decisions: Fairness, trust, and emotion in response to algorithmic management. Big data & society, 5(1), 2053951718756684.
    </a>
    <p>
        Lee examines job applicants’ reactions to AI recruitment decisions, finding that perceptions of fairness and trust are deeply affected when efficiency is prioritised without ethical safeguards. The paper critiques how AI recruitment tools can undermine justice as fairness and deontological principles if transparency and respect for individuals' rights are not maintained.
    </p>
    <br>
    <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372828">
        Raghavan, M., Barocas, S., Kleinberg, J., & Levy, K. (2020, January). Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 469-481).
    </a>  
    <p>
        Raghavan and colleagues analyse existing practices aimed at addressing bias in algorithmic hiring, critiquing the trade-offs between efficiency and fairness. They argue that hiring systems focused solely on efficiency metrics risk undermining equity, trust, and privacy, contradicting Rawlsian justice and deontological duties to avoid harm and discrimination.
    </p>
    <br>
    <a href="https://dl.acm.org/doi/abs/10.1145/3306618.3314244">
        Raji, I. D., & Buolamwini, J. (2019). Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society (pp. 429-435).
    </a>
    <p>    
        The authors report on how commercial artificial intelligence recruiting tools show biased performance, especially against women and people of color.  The report underlines how public responsibility and independent audits help to reduce prejudice in AI-powered hiring processes.
    </p>
    <br>
    <a href="https://www.taylorfrancis.com/chapters/edit/10.4324/9781315097176-4/theory-justice-john-rawls">
        Rawls, J. (2017). A theory of justice. In Applied ethics (pp. 21-29). Routledge.
    </a>
    <p>
        John Rawls's foundational work on justice as fairness offers a theoretical framework for assessing moral hiring policies.  His ideas of justice and equal opportunity provide a structure for evaluating the moral consequences of hiring systems controlled by artificial intelligence.
    </p>
    <br>
    <a href="https://dl.acm.org/doi/abs/10.1145/3351095.3372849">
        Sánchez-Monedero, J., Dencik, L., & Edwards, L. (2020, January). What does it mean to'solve'the problem of discrimination in hiring? Social, technical and legal perspectives from the UK on automated hiring systems. In Proceedings of the 2020 conference on fairness, accountability, and transparency (pp. 458-468).
    </a>
    <p>
        This paper focuses on AI recruitment systems in the UK and how attempts to optimise efficiency can create ethical dilemmas around discrimination and fairness. The authors argue that algorithmic hiring often fails to satisfy justice as fairness and deontological standards, suggesting that merely optimising technical performance cannot address deeper moral concerns in hiring.
    </p>
</div>
{% endblock %}